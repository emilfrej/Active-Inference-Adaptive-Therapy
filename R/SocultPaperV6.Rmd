---
title: "SocultPaperV6"
output:
  word_document: default
  pdf_document: default
date: "2024-05-19"
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, fig.height = 3, fig.width = 5, fig.align = "center")
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())


# set the directory to visualizations folder


pacman::p_load(tidyverse, ggnewscale, patchwork)
theme_set(theme_classic())


#set colors
rb_col <- "green"
long_col <- "red"
medium_col <- "skyblue"
short_col = "violet"

strat_colors <- c("RB" = rb_col, "Long" = long_col, "Medium" = medium_col, "Short" = short_col)
contrast_colors <- c("long_vs_rb" = long_col, "medium_vs_rb" = medium_col, "short_vs_rb" = short_col)

source("ReadInData.R")

```

# Introduction

### Adapative Therapy

Worldwide, cancer is the cause of 1 out of 6 deaths. An estimated 90% of these cancer deaths are due to the development of drug resistance [@bukowski2020]. While initial cancer treatment usually shows a positive response in tumor burden, drug resistance develops over time. To highlight the inefficiency of traditional approaches, [@stanková2019] models cancer treatment as a game-theoretic contest between a physician and a tumor. In this model, the physician's move in each round is to apply a certain treatment, and to which the tumor adapts. While it is a "Stackelberg game"—a game where one player is the leader (the physician) and another player is a follower (the tumor)—this asymmetry is rarely exploited in oncology wards. Instead of using their advantage to steer the evolutionary pressures placed on tumors, physicians let the tumor adapt not only to the current round of the game but also to future rounds. Thus, the advantage of leading the game is lost. The authors analogize the current practice to a game of rock-paper-scisccors:

> *" in which almost all cells within the cancer play, for example, “paper.” It is clearly advantageous for the treating physician to play “scissors.” Yet, if the physician only plays “scissors,” the cancer cells can evolve to the unbeatable resistance strategy of “rock.”"* [@stanková2019]

To exploit this assymetry *Adaptive Therapy* (AT) controls the intra-tumoral evolutionary dynamics by leveraging a likely fitness costs that cancer cells incur when evolving mechanisms to resist drugs. For example, it has been shown that tumor cells can mutate to increase the expression of the PGP membrane pump, which uses ATP to expel drugs from the cell. While this makes the cell more resistant to treatment, it also comes at a metabolic cost. @gatenby2009 found that PGP activity was responsible for approximately 50% of cell metabolism. If resistant and non-resistant cells are competing for space and resources, drug-sensitive cells should, over time, outcompete resistant cells since they can devote more metabolism to cell proliferation. Thus, AT utilizes Darwinian competition to make cancer suppress itself, a strategy that potentially could make cancer a controllable chronic disease.

![From Zhang et al. 2023. Panel A shows a typical "Maximal Tolerable Dose" treatment protocol. While the initial response in tumor burden is promising, a compettive release of resistant cells ensures. Panel B shows AT approach with a small tumor burden. Panel C shows AT with a high tumor burden, which has been theorized to further increase the suppression of resistance cells through darwinian competition.](images/Screenshot%202024-05-20%20at%2008.15.19.png){width="400"}

Initial results from an on-going pilot clinical trial applying AT in a group of *metastatic castrate-resistant prostate cancer* (mCRPC) patients are promising, showing both lower cumulative dosages and longer survival times compared to a similar group of patients receiving standard care. The trial utilizes a range-bounded heuristic to make treatment decisions: if the blood marker *Prostate-specific Antigen* (PSA), a proxy for tumor burden, increases back to pre-trial levels, Abiraterone treatment is applied until PSA drops to 50% of pre-trial levels [@zhang2017]. The trial is expected to run until December 2024 [@h.leemoffittcancercenterandresearchinstitute2024].

### Needed Developments in Mathematical Modeling 

Key AT researchers @west2023 have recently surveyed the use of mathematical modeling in the discipline and identified needed developments to realize the potential of AT. For example, they argue that modelling approaches will need the capacity to simulate multidrug therapies. Additionally, they argue that it is unlikely that any single treatment approach will be able to delay the emergence of resistant cells, while lowering tumor burden and minimizing toxicity. Given that patients likely differ in their ability to tolerate tumor burden, drug toxicity, and the evolutionary dynamics of their cancers, therapies will have make informed trade-off between these objectives for each individuaal patient. The authors also report several hirthetho poorly understood biological mechanisms, such as how competition dynamics change depending on carrying capacities of tumors, or how high tumor burdens might be detrimental due to increased risks of new metastasis. More complicated modelling of tumor dynamics is therefore required, and future modelling approaches will require flexibility in their ability specify complex causal systems.

Predicting real-time treatment responses of individual patients would likely greatly enhance AT efficacy, since dose modulation could be individualized further to control intratumoral evolutionary dynamics with greater precision. A key issue going forward is rethinking how real-time patient data is collected. @west2023 stress the importance of collecting data not only to detect progression but also to inform future treatment decisions. For example, a risk of using a set testing frequency, as done in the pilot clinical trial, is treatment *overshooting*. In trial, drops in PSA weren't always detected early enough to minimize induced resistance. This could have limited future treatment opportunities. Additionally, whatever the modelling approach of choice, the authors underline that it allows for careful analysis of the uncertainty in everything from estimated tumor dynamics to treatment outcomes.

### Partially Observable Markov Decision Process

*Partially Observable Markov Decision Processes* (POMDPs) are a class of controller models that model an Markov process. An environment moving through discrete times and states, for which next step of the system only depends on the current step. Partial observability refers to the fact that these models don't directly observe the actual environment but instead only noisy signals emitted its changing states [@åström1969]. This allows POMPDs to differentiate an observed signal from what it "believes" about the actual configuration of states and additionally to use a single reward function to trade-off uncertainty for reaching a certain goal state [@kaelbling1998].

### Solving POMPDs with AI

While POMDPs are typically difficult to solve analytically, various approximate approaches exist. One approximation scheme is born out of the neuroscience paradigm *Active Inference* (AI). Typically used to model an agents decision making. The approxiamtion scheme finds a solution by minimizing two objective functions:

-   *Variational Free Energy* (VFE). A measure of fit between a generative model and sensory input.

-   *Expected Free Energy* (EFE). A score of how well a course of action is expected to bring about a set of preferences.

AI has been used to model psychopathology [@dacosta2020] but also applied to control scenarios such as the mountain-car problem [@friston2009] and, albeit augmented with deep-learning, robotics control [@çatal2020]. AI implementations of the POMDP scheme have been implemented in MATLAB and recently in Python with the python package *pymdp* [@heins2022 and @heins2022a].

Typically, AI POMDPs are technically a joint probability:

$$
p(o,s,u;\phi)
$$

where $o$ are observations, $s$ are hidden states, $u$ are "control states" (actions that an agent can take to influence the enviorment), and $\phi$ are the hyperparameters of the model, such as $\alpha$ typically used as 'inverse temperature' i.e. how deterministically the model selects actions. By conditioning on certain observations, the POMDP is solvable by approximation for bayes-optimal posterior beliefs at a given time point. This yields a set of posterior beliefs not only about the configuration of each modeled type of state in the environment, but also posterior probability distribution for the optimal course of action to achieve a set of preferences given the expected uncertainty. As an example, a POMDP could model the joint probability of observing a noisy signal of partiuclar tumor burden in a patient, an actual underlying tumor burden, and the course of action that is most likely to bring the tumor burden down. By modelling modelling, actions, states and observations as discrete events, the joint probability can be made tractable by factorized for tractability into the following categorical probability distributions:

-   The likelihood model, $A$. $A$ is typically a set of arrays where each array corresponds to a *modality* — a category of observations. The each cell of the array describes the likelihood of making a particular observation in the modality given some configuration of states. For example, PSA readings could be considered a modality, and different test results would be the observations within the modality. The likehoods within the modality could then be modeled as a perfect signal of ttumor burden, i.e., the same tumor burden always gives the same test result, or as a noisy signal.

-   The transition model, $B$, describes the probability of each possible stransitions of within a type of state between timesteps. $B$ also encodes how actions are expected to influence the transition probabilities between states. For example, it could describe how a tumor is likely to evolve from one timestep to the next depending on whether treatment is being applied. The transition model is usually coded as a collection of three-dimensional arrays, a first dimension for the next state, a dimension for the current state, and a third dimension with the length of each action that would influence transition probabilities of the state.

-   A prior over preferred states $C$. A particularity of AI POMPDs is that utility is a probabilities for making certain observations. It can be considered the states that the system attempts to "self-organize" around. This method of modelling preferences can superficially seem odd, but its benefits will be clear later.

-   A prior over initial states $D$. These are vectors of probabilities specifying intial beliefs. In continuation with example above, $D$ could specify how probable a model believes different levels of tumor burden are before making any observations. When AI POMPDs are solved on multiple time-steps, the posterior beliefs of the preceding time-step replaces $D$.

Minimzing VFE and EFE to solve a given POMPD for a given timepoint also requires approximating the Free Energies. This is done by a neuronally inspired method known as "message-passing", for which multiple implementations exists [@smith2022]. [^1]

[^1]: The Python package pymdp [@heins2022] has numerous tutorial notebooks for implementing POMPDs in python. @smith2022 describes the process of the constructing POMPDs for different scenearios in detail, albeit in MATLAB and @dacosta2020 provides in-depth mathematical account of POMPDs in Active inference.

## The aim of this paper

The treatment decisions used by in the pilot trial reported by @zhang2017 were based on a well-informed heuristic about how resistance dynamics would develop as consequence of careful timing of applying and withdrawing treatment. However, the actual resistance dynamics of each patient wasn't explicitedly modeled real-time, and thus not used in treatment decisions.

This paper will investigate whether the AI implementation of POMDPs could viably model the degree of treatment resistance of individual cancer patients to improve long-term control of the disease. Beyond fitting certain desiderata reported by [@west2023], the use of the AI paradigm for the present project was further motivated by the following considerations:

-   Real-time data is likely sparse in clinical settings. Extracting the maximum amount of information from each data point will therefore be essential. Using models that perform optimal Bayesian inference would therefore be preferable.

-   Short-term exploitation of tumor vulnerability is essential for keeping the patient alive, but for long-term success, controlling resistance dynamics will be crucial. A therapy plan will have to balance keeping the patient alive now against limiting its future options for treatment. Using a measure such as Expected Free Energy could be instrumental in striking this balance.

-   Resistance dynamics are not directly measurable, but should be inferable through observing how the tumor burden responds to treatment. Choosing whether to treat would therefore not only be a consideration of the tumor level but also about how much it informs us about the resistance dynamics.

-   The information gain from testing can be quantified and used to prioritize testing for periods of greater uncertainty.

An interesting corollary to these motivations is that AI POMDPs could suggest some superficially counterintuitive treatment decisions. For example, a model could correctly estimate that one patient has a low tumor burden and suggest not applying treatment, while proposing treatment for another patient with exactly the same burden. This is a feature, not a bug. If the model has less certainty about the resistance dynamics of the second patient, applying treatment could be the optimal move simply to gain information about the resistance dynamics of the cancer in the second patient.

# Analysis

## Simulated environment

To investigate the appropriateness of the AI implementation of the POMDP scheme, multiple simulations of a clinical setting inspired by the pilot clinical trial were run. To comply with the canonical AI POMPD scheme discrete state and time, discrete states and time were used. In each simulation run, a simulated a maximum of 200 time-steps was created. At each time-step, an agent has to keep a virtual cancer patient alive by deciding when to apply a treatment based on a signal of the tumor burden. The signal could be observed at each time point, and treatment decisions were made by solving a slightly modified AI POMPD. The features of the eviroment was inspired by the use of PSA testing and Abiraterone treatment in the pilot trial [@zhang2017]. In the simulations, the patient's *tumor state* determined whether they survived to the next time-step. If the tumor state every reached maximal state of six possible tumor states, the virtual patient "died" and simulation ended. The tumor was always set to 0 at the beginning of each simulation run, and could increase at a fixed risk at each time-step.

Whether a round of treatment successfully reduced the tumor state depended on an underlying *resistance state*. This also began at state 0 out of 5. For each round of treatment application, the resistance state had a fixed risk of increasing too. This rendered treatment increasingly ineffective as time progressed. The only remedy, beyond favorable bouts of stochasticity, was to bring down the resistance state by withdrawing treatment. Inspired by the work of [@hansen2020], higher tumor levels would increase the chance of bringing down the resistance level at each step.

In summary, this means that the simulated agents had balance the tumor state not growing out of control, while not painting itself into a corner by applying treatment too frequently. Striking this balance was further complicated by the stochasticity of the evolving tumor-burdens, changes to resistance levels and realized treatment-responses.

## Modifications to POMDP scheme

Typically for AI POMDPs, different types of hidden states such as resistance and tumor states are not modeled to affect each other directly. Instead, the interactions of combinations of different states are modeled as leading to different observations. This means that interactions between *types of states* would be coded in the likelihood model $A$.

This setup was deemed incompatible with the clinical setting attempted emulated, since it wouldn't let let a model infer how changes to resistance state should influence the expected transition probabilities of the tumor state. Therefore, custom changes to pymdp were made to allow POMPDs to infer the state of "occluded" nodes – causally upstream nodes that don't themselves produce signals (e.g. the resistance state). Instead the state of the occluded node would have to be inferred, through its effects on downstream nodes that generate signals (e.g. the tumor state).

Specifically, modifications were made to the structure of $B$, the generative model's transition beliefs. Usually, three-dimensional *B-tensors* describe expected transition probabilities within type of state state. This means a B-tensor for type of state typically holds one dimension for the current state, one dimension for the next state, and a third dimension for every allowed action. Each cell of the tensor then contains the transition probability from one state to another conditional on a certain action. Another dimension was added to relevant b-tensors which corresponded to another type of state. Concretely, this meant that the tumor b-tensor had a fourth dimension corresponding to the possible resistance states. The expected transition probabilities could then be estimated by matrix multipliyng the tumor state B- with a vector the posterior of expected resistance at a given time point. One can imagine that instead of having only one B-tensor for the transition probabilities of the tumor state, multiple B-tensors of tumor transition probabilities were specified, were each tensor corresponded to a specific resistance level. This modification to the generative model necessitates taking an average of all these tensors weighted by the posterior probabilities for each resistance state at given time point. This amounts to calculating the expected tumor state conditional on a set of beliefs about the resistance state. This modification strategy was repeated for the resistance level, since decreases in the resistance state depended on the tumor state. It should be noted that this modification means that the order in which beliefs about states are evaluated must be specified. For example, for the present simulations, the effect of resistance level on treatment efficacy was evaluated first since this was evaluated first for each step of the simulation, while the transition probabilities between resistance states were evaluated after an action had been taken.

While these changes rendered much of the functionality of pymdp immediately unusable, the modification strategy should be able to model arbitrarily complex dependencies between states. This could potentially allow POMPDs to model more complex biological mechanism as desired by @west2023. Hopefully, a robust implementation can be designed without overhauling pymdp.

## Exploratory Simulation

An exploratory simulation was conducted, where for each run an agent instiatied as a modified POMPD controlled whether to treat and test. Its modalities were:

-   A noiseless signal of whether the patient was alive.

-   A noiseless signal of wheter it was applying tests.

-   A noiseless signal whether it was applying treatment

-   A noised signal of the tumor state, but only if tests were being applied.

The agent's prior preferences were skewed heavily against observing a dead patient, somewhat against treating, and slightly against testing. This was done to simulate a cost to both treating and testing. This means that the agent had to balance the cost of its actions while considering the potential information gain of each choice. The costs to treating and testing were introduced to investigate whether the POMPD scheme potentially could be used to strike a balance between drug toxicity, tumor burden while optimizing real-time data collection as desired by @west2020. The model was further handicapped by noise in the tumor signal. This means that the resistance state, which had to be inferred through the tumor signal, was doubly obfuscated.

However, the generative model used did perfectly map the expected noise in the tumor signal, and modeled transition probabilities identical to the actual transition probabilities of the enviroment. Additionally, the agent was given uniform priors over initial states, meaning it had no knowledge of the configuration of states at the beginning of each run. The same set of predetermined policies that considered the next `r num_policies` timesteps was evaulated at each step. This limited set of policies was done to ease computation by limiting the search space of possible actions. The set of policies consisted of two blocks of either testing or treating for three timesteps in a row for permuted by each possible sequence of applying testing or not for the time horizon of the policies.

### Learning the Resistance State

On a the following example of a explotaroy simulation run, the agent managed to keep the patient alive for `r exploratory_sim_length` timesteps and chose to test on `r num_tests` of these (see Fig. 2).

```{r fig.cap= "A simulation run for exploratory purposes."}
source("sim_run_example.R")
plot
```

The agent appears to be somewhat sensibly applying treatment, but it doesn't apply treament at pivotal period approximately between steps 30 and 39. Generally, it refrains from applying treatment when the tumor state is low, and applies treatment when the tumor state is high. Interestingly, it does applies treatment at the first timestep, despite the tumor state and resistance state being at their lowest possible values. Considering that the generative model had uniform priors over both resistance state and tumor states, this behavior is more appropriate from the perspective of the agent, since treating and testing would immediately provide information about both states. Qualitatively, the model also seems to prefer testing when treatment is being applied. This suggests that it finds testing to be more worthwhile while also treating. A plot of the model's beliefs about the resistance state at each time-point (see Fig. 3) shows that its beliefs about the resistance state generally follow the development of the actual resistance state despite the lack of direct signal.

```{r, fig.cap="Plot of the models belief about the reistance level at every timepoint"}

source("resistance_beliefs_plot.R")
plot 
```

In summary, the modification to POMPD structure appears to have allowed the agent to otherwise occluded resistance state by selectively applying treatment and collecting noised signals of the tumor burden. While the results are intially promising, the particular agent does make questionable choices regarding the timing of treatment. The decision-making behind these choices is dissected in-depth the appendix.

## Performance of POMPDs against a Range-Bounded Strategy

Several agents instiantied as modified POMPDs were tested against a range-bounded strategy. This was done to benchmark the performance of POMPDs against a strategy inspired by the treatment protocol used in the pilot clinical trial [@zhang2017]. The generative models used were simplified for ease of computation POMPDs and as result they observed a noiseless signals of the tumor state and whether treatment was being applied at every timestep. They were also only given prior preferences for observations of tumor states. The preferences were uniform over all survivable tumor states, and the probability of the highest tumor state was set to 0. The following three agents were tested:

-   A short policy horizon POMPD that could consider a block of treating or not treating for the next three steps.
-   A medium policy horizon POMPD that could consider two blocks of treating or not treating for three steps, yielding a total horizon of 6 timesteps.
-   A long policy horizon model that could consider four blocks of treating or not treating for three steps, resulting in a total horizon of 12 steps.

The agents were benchmarked on four different "tumor aggressivity settings". The tested tumor growth rates were 0.1, 0.3, 0.5, and 0.7 probability of increasing the tumor state at each timestep. For each setting, 100 simulation runs was done. For each run, the environment was predetermined by generating 200 outcome variables for each combination of tumor state, resistance state, and treatment state to ensure comparability between the tested agents and the RB-strategy at the level of each run.

```{r, fig.cap= "Simulated patient survival times for each agent and baseline strategy."}
#print densities
source("sim_lengths_densities_full.R")

plot + 
  #set color scheme so that long _vs_adt is green
  scale_fill_manual(values = strat_colors) +
  scale_color_manual(values = strat_colors) 



```

At the lowest tumor growth rate, nearly all the runs reached the maximal length (see Fig. 7). The longest policy strategy also appeared to perform slightly better than the other models. Since the difficulty of managing the tumor burden changed, contrasts were computed between the long policy POMPD and the range-bounded strategy. Because the stochasticity of each run was predetermined, the percentage difference from switching from an RB-strategy to a POMPD can be computed for each run (see Fig. 8).

```{r, fig.cap="Figure 8. Shows the change in percent of between the range-bound approach and hte longest horizon POMPD for tumor aggresivity settings"}
source("contrasted_sim_length_max_horizon.R")

plot +
  scale_fill_manual(values = contrast_colors) +
  scale_color_manual(values = contrast_colors) +
  xlim(-50, 400)

```

In the 100 runs with a 0.1 risk of tumor growth, the long horizon POMPD seems to perform similarly to the RB strategy. However, as the tumor growth risk increases, the POMPD model increasingly shows large percentage improvements compared to the RB strategy. This could be because, as the tumor dynamics speed up, the need for proactive action becomes more important, favoring models that attempt to also control the underlying resistance state.

# Discussion

## What This Project Found

This project tested a simple modification to how POMDPs are typically implemented in AI to investigate whether POMDPs could learn and control a hidden state that could only be inferred based on its consequences on other states. This was done in a simulated environment designed to mimic the dynamics of AT. The modification was successful, allowing the POMDP to model an underlying resistance state that controlled the efficacy of treatment, despite the underlying state not producing any observable signals itself. POMPDs were further tested against a range-bounded treatment strategy and were found to outperform the range-bounded strategy. The difference in performance increased as the aggressiveness of tumor growth increased and as the future time-horizon that the POMPDs considered at each time step lengthened. This finding underscores the potential of using the AI paradigm to plan AT, and particularly demonstrates that, under simplified circumstances, POMDPs could be a useful choice for implementing AI for real-time treatment decision-making.

Additionally, this paper demonstrates that the AI implementation of POMPDs shows several desirable qualities for AT POMDPs:

-   The models are flexible. Computation and our ability to inform likelihood and transition dynamics are the only limits to the combinations of hidden states, actions, and outcome modalities. This paper implemented a POMPD capable of combining multiple decisions, such as treating and testing, but more complex generative models could include multiple testing and treatment actions. As [@west2023] highlight, it is important to incorporate biological factors into modeling decisions.

-   Another quality is the capacity to balance expected information gain against expected utility. The certainty of the model is incorporated directly in selecting when to apply treatments and when to test. This suggests that there is more information to be gained from testing when treatment is being applied, as it provides a view into the underlying resistance dynamics. Immediate testing seems to suggest that the models are more likely to apply testing when also applying treatment. Another interesting corollary is that a POMPD could believe that two patients have exactly the same tumor burden but suggest treatment for one and not the other. This is a feature, not a bug. If the model is certain about an underlying state (e.g., resistance in the models in this paper), there is less gain in information from treating. On the other hand, if the model is uncertain about the resistance state, it might be worthwhile to treat simply to gain information. This is appropriate behavior, as it allows the model to better its chances of keeping the patient alive in the future.

-   Another quality desired [@west2023] is rigorous uncertainty. Since the POMDP's certainty about current and future states is easily extractable, this criterion is arguably met. Additionally, its "reasoning," i.e., free energy estimates, can be examined for each action.

## Future Research

While the results are initially promising, the simple nature of the simulation prohibits drawing any strong conclusions about whether AI POMPDs could eventually be a good model choice in a clinical setting.

To further investigate whether POMDPs could apply to a clinical setting, multiple problems will need to be solved. For example, in the present study, the environment featured only discrete values. Since biomarkers and dosage intensities will likely be continuous values, some combination of binning continuous values to discrete values or adapting the model structure to work with continuous data is necessary. Deep learning has been used to construct likelihood mappings and transition probabilities in POMDPs from continuous data [@çatal2020]. Another key issue is determining a way to inform likelihood and transition probabilities in a fashion that would be viable in a clinical setting. The present study simply used the actual transition probabilities of the simulated environment and, depending on the simulation run, also used noiseless likelihood mappings. Possibly, simulations of cancer dynamics, such as those by [@zhang2017], could be used to inform transition probabilities, and a clinical model could potentially readjust to patient data. The AI implementation of POMDPs has a developed literature on learning probabilities from data and even on how to incorporate the information gain of learning transition and likelihood probabilities into decision-making ([@smith2022] and @dacosta2020]).

Another key issue is constructing a more rigorous benchmarking system for proposed models. In the present paper, the ranges of the compared range-bounded model were selected fairly arbitrarily. It produced convincing results during initial testing and did manage to control tumor levels for a substantial amount of time. However, a systematic method of comparing models is necessary, especially considering the difficulty of real-world tests. No matter the choice of modeling framework, building a benchmarking suite is crucial for the AT discipline. Given the difficulty of real-world testing, we must maximize the information that can be extracted from simulation work. [@west2023] has produced a detailed qualitative account of needed developments in mathematical models. Translating this account to a set of simulation environments would be extremely useful. A number of useful simulation scripts probably already exist. For example, [@zhang2017] released their simulations in Matlab. Wrapping existing simulations into environments that can easily exchange actions and observations could be facilitated by using an API like the one used in Gymnasium (formerly OpenAI Gym) [@towers2024] to minimize friction for non-oncology researchers. If experts in AT were to predetermine a set of benchmarks, it would also greatly ease the burden on outsiders to identify useful contributions, allowing them more time to implement these contributions.

If an easy-to-use benchmarking suite existed, POMPDs could also be "reverse-engineered" from more complex models. If a black-box model, like a neural network, can be shown to successfully control treatment application in more complicated simulations of cancer dynamics, this would presumably not be too difficult considering that POMPDs have often been fitted to human decision-making in computational psychiatry. The same techniques could potentially allow us to translate the decision-making of a black-box model into the structure of a POMPD, thus combining the performance of the black-box model with the transparency of POMPD structure.

# Appendix A - A Dissecting an Agents Behavior.

### Expected trajectories

For each the policy, the agent evaluates the expected trajectory of the hidden states (see Fig. x). These evaluations provide insights into the model's uncertainty and what it expects to happen under the best policy at each time point.

```{r}
source("single_model_trajectories plot.R")
plot
```

The amount of "flip-flopping" on decisions can also be investigated by examining what policy the model finds the most promising at each time step (see Fig. 4).

```{r}
source("single_run_overall_fep_policies.R")
plot
```

Curiously, there is a long stretch, approximately from timesteps 30 to 39, where the model never considers treating to be the optimal course of action, even though the resistance level is low. The model also holds accurate beliefs about the resistance level at this timepoint (see Fig. 2). However, this is likely due to a combination of multiple factors. First, there is a cost to treating, which means that the model generally prefers not to treat. It also underestimates the tumor burden at this timepoint (see Fig. 5) and gets "caught off guard" by a sudden rise in the tumor level. Since there is a cost associated with testing, it is also hesitant to apply tests during this period. Because it only considers six timesteps into the future, it likely doesn't consider the long-term consequences of not reducing the tumor level.

```{r}
source("SingleTrial_tumor_beliefs.R")
plot

```

The exact "decision making" for each policy can be evaluated. Each policy is evaluated for its EFE, which is a combination of the expected utility penalized for expected uncertainty (see Fig. 6).

```{r}

source("FEPcomponents.R")
plot
```

The free energy components reveal several interesting decision-making points. At first, the model, given its uniform priors, is very uncertain about outcomes, and decreasing uncertainty weighs heavily in its decision making. This uncertainty also means that the model believes it is more likely to be in a risky situation, and reducing this risk is a priority. During the earlier identified pivotal period between timesteps 30-39, there is a concerning lack of increase in the weight of the utility component. This could mean that the model is too concerned with avoiding both treating and testing. It is only from timestep 40 onwards that the model realizes the severity of the situation, i.e., the rising tumor level (see Fig. 5). This analysis warrants strategies to increase the model's ability. Changing the prior preferences to make testing and treatment "cheaper" in terms of utility might help. However, simply treating and testing more would translate to actual costs, both financial and to patient well-being. While it is important that the prior preferences are tuned to costs in a clinical setting, it would probably be more fruitful to give the model longer policies to consider. This would hopefully yield models that are better tuned to the risks of not detecting and controlling tumor levels at low values.
