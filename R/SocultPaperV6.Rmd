---
title: "SocultPaperV6"
output:
  word_document: default
  pdf_document: default
date: "2024-05-19"
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, fig.height = 3, fig.width = 5, fig.align = "center")
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())


# set the directory to visualizations folder


pacman::p_load(tidyverse, ggnewscale, patchwork)
theme_set(theme_classic())


#set colors
rb_col <- "green"
long_col <- "red"
medium_col <- "skyblue"
short_col = "violet"

strat_colors <- c("RB" = rb_col, "Long" = long_col, "Medium" = medium_col, "Short" = short_col)
contrast_colors <- c("long_vs_rb" = long_col, "medium_vs_rb" = medium_col, "short_vs_rb" = short_col)

source("ReadInData.R")

```

# Introduction

### Adapative Therapy

Worldwide, cancer is the cause of 1 out of 6 deaths. An estimated 90% of these cancer deaths are due to the development of drug resistance [@bukowski2020]. While initial cancer treatment usually shows a positive response in tumor burden, drug resistance develops over time. To highlight the inefficiency of traditional approaches, [@stanková2019] models cancer treatment as a game-theoretic contest between a physician and a tumor. In this model, the physician's move in each round is to apply a certain treatment, and to which the tumor adapts. While it is a "Stackelberg game"—a game where one player is the leader (the physician) and another player is a follower (the tumor)—this asymmetry is rarely exploited in oncology wards. Instead of using their advantage to steer the evolutionary pressures placed on tumors, physicians let the tumor adapt not only to the current round of the game but also to future rounds. Thus, the advantage of leading the game is lost. The authors analogize the current practice to a game of rock-paper-scisccors:

> *" in which almost all cells within the cancer play, for example, “paper.” It is clearly advantageous for the treating physician to play “scissors.” Yet, if the physician only plays “scissors,” the cancer cells can evolve to the unbeatable resistance strategy of “rock.”"* [@stanková2019]

To exploit this assymetry *Adaptive Therapy* (AT) controls the intra-tumoral evolutionary dynamics by leveraging a likely fitness costs that cancer cells incur when evolving mechanisms to resist drugs. For example, it has been shown that tumor cells can mutate to increase the expression of the PGP membrane pump, which uses ATP to expel drugs from the cell. While this makes the cell more resistant to treatment, it also comes at a metabolic cost. @gatenby2009 found that PGP activity was responsible for approximately 50% of cell metabolism. If resistant and non-resistant cells are competing for space and resources, drug-sensitive cells should, over time, outcompete resistant cells since they can devote more metabolism to cell proliferation. Thus, AT utilizes Darwinian competition to make cancer suppress itself, a strategy that potentially could make cancer a controllable chronic disease.

![From Zhang et al. 2023. Panel A shows a typical "Maximal Tolerable Dose" treatment protocol. While the initial response in tumor burden is promising, a compettive release of resistant cells ensures. Panel B shows AT approach with a small tumor burden. Panel C shows AT with a high tumor burden, which has been theorized to further increase the suppression of resistance cells through darwinian competition.](images/Screenshot%202024-05-20%20at%2008.15.19.png){width="400"}

Initial results from an on-going pilot clinical trial applying AT in a group of *metastatic castrate-resistant prostate cancer* (mCRPC) patients are promising, showing both lower cumulative dosages and longer survival times compared to a similar group of patients receiving standard care. The trial utilizes a range-bounded heuristic to make treatment decisions: if the blood marker *Prostate-specific Antigen* (PSA), a proxy for tumor burden, increases back to pre-trial levels, Abiraterone treatment is applied until PSA drops to 50% of pre-trial levels [@zhang2017]. The trial is expected to run until December 2024 [@h.leemoffittcancercenterandresearchinstitute2024].

### Needed Developments in Mathematical Modeling 

Key AT researchers @west2023 have recently surveyed the use of mathematical modeling in the discipline and identified needed developments to realize the potential of AT. For example, they argue that modelling approaches will need the capacity to simulate multidrug therapies. Additionally, they argue that it is unlikely that any single treatment approach will be able to delay the emergence of resistant cells, while lowering tumor burden and minimizing toxicity. Given that patients likely differ in their ability to tolerate tumor burden, drug toxicity, and the evolutionary dynamics of their cancers, therapies will have make informed trade-off between these objectives for each individuaal patient. The authors also report several hirthetho poorly understood biological mechanisms, such as how competition dynamics change depending on carrying capacities of tumors, or how high tumor burdens might be detrimental due to increased risks of new metastasis. More complicated modelling of tumor dynamics is therefore required, and future modelling approaches will require flexibility in their ability specify complex causal systems.

Predicting real-time treatment responses of individual patients would likely greatly enhance AT efficacy, since dose modulation could be individualized further to control intratumoral evolutionary dynamics with greater precision. A key issue going forward is rethinking how real-time patient data is collected. @west2023 stress the importance of collecting data not only to detect progression but also to inform future treatment decisions. For example, a risk of using a set testing frequency, as done in the pilot clinical trial, is treatment *overshooting*. In trial, drops in PSA weren't always detected early enough to minimize induced resistance. This could have limited future treatment opportunities. Additionally, whatever the modelling approach of choice, the authors underline that it allows for careful analysis of the uncertainty in everything from estimated tumor dynamics to treatment outcomes.

### Partially Observable Markov Decision Process

*Partially Observable Markov Decision Processes* (POMDPs) are a class of controller models that model an Markov process. An environment moving through discrete times and states, for which next step of the system only depends on the current step. Partial observability refers to the fact that these models don't directly observe the actual environment but instead only noisy signals emitted its changing states [@åström1969]. This allows POMPDs to differentiate an observed signal from what it "believes" about the actual configuration of states and additionally to use a single reward function to trade-off uncertainty for reaching a certain goal state [@kaelbling1998].

### Solving POMPDs with AI

While POMDPs are typically difficult to solve analytically, various approximate approaches exist. One approximation scheme is born out of the neuroscience paradigm *Active Inference* (AI). Typically used to model an agents decision making. The approxiamtion scheme finds a solution by minimizing two objective functions:

-   *Variational Free Energy* (VFE). A measure of fit between a generative model and sensory input.

-   *Expected Free Energy* (EFE). A score of how well a course of action is expected to bring about a set of preferences.

AI has been used to model psychopathology [@dacosta2020] but also applied to control scenarios such as the mountain-car problem [@friston2009] and, albeit augmented with deep-learning, robotics control [@çatal2020]. AI implementations of the POMDP scheme have been implemented in MATLAB and recently in Python with the python package *pymdp* [@heins2022 and @heins2022a].

Typically, AI POMDPs are technically a joint probability:

$$
p(o,s,u;\phi)
$$

where $o$ are observations, $s$ are hidden states, $u$ are "control states" (actions that an agent can take to influence the enviorment), and $\phi$ are the hyperparameters of the model, such as $\alpha$ typically used as 'inverse temperature' i.e. how deterministically the model selects actions. By conditioning on certain observations, the POMDP is solvable by approximation for bayes-optimal posterior beliefs at a given time point. This yields a set of posterior beliefs not only about the configuration of each modeled type of state in the environment, but also posterior probability distribution for the optimal course of action to achieve a set of preferences given the expected uncertainty. As an example, a POMDP could model the joint probability of observing a noisy signal of partiuclar tumor burden in a patient, an actual underlying tumor burden, and the course of action that is most likely to bring the tumor burden down. By modelling modelling, actions, states and observations as discrete events, the joint probability can be made tractable by factorized for tractability into the following categorical probability distributions:

-   The likelihood model, $A$. $A$ is typically a set of arrays where each array corresponds to a *modality* — a category of observations. The each cell of the array describes the likelihood of making a particular observation in the modality given some configuration of states. For example, PSA readings could be considered a modality, and different test results would be the observations within the modality. The likehoods within the modality could then be modeled as a perfect signal of ttumor burden, i.e., the same tumor burden always gives the same test result, or as a noisy signal.

-   The transition model, $B$, describes the probability of each possible stransitions of within a type of state between timesteps. $B$ also encodes how actions are expected to influence the transition probabilities between states. For example, it could describe how a tumor is likely to evolve from one timestep to the next depending on whether treatment is being applied. The transition model is usually coded as a collection of three-dimensional arrays, a first dimension for the next state, a dimension for the current state, and a third dimension with the length of each action that would influence transition probabilities of the state.

-   A prior over preferred states $C$. A particularity of AI POMPDs is that utility is a probabilities for making certain observations. It can be considered the states that the system attempts to "self-organize" around. This method of modelling preferences can superficially seem odd, but its benefits will be clear later.

-   A prior over initial states $D$. These are vectors of probabilities specifying intial beliefs. In continuation with example above, $D$ could specify how probable a model believes different levels of tumor burden are before making any observations. When AI POMPDs are solved on multiple time-steps, the posterior beliefs of the preceding time-step replaces $D$.

Minimzing VFE and EFE to solve a given POMPD for a given timepoint also requires approximating the Free Energies. This is done by a neuronally inspired method known as "message-passing", for which multiple implementations exists [@smith2022]. [^1]

[^1]: The Python package pymdp [@heins2022] has numerous tutorial notebooks for implementing POMPDs in python. @smith2022 describes the process of the constructing POMPDs for different scenearios in detail, albeit in MATLAB and @dacosta2020 provides in-depth mathematical account of POMPDs in Active inference.

## The aim of this paper

The treatment decisions used by in the pilot trial reported by @zhang2017 were based on a well-informed heuristic about how resistance dynamics would develop as consequence of careful timing of applying and withdrawing treatment. However, the actual resistance dynamics of each patient wasn't explicitedly modeled real-time, and thus not used in treatment decisions.

This paper will investigate whether the AI implementation of POMDPs could viably model the degree of treatment resistance of individual cancer patients to improve long-term control of the disease. Beyond fitting certain desiderata reported by [@west2023], the use of the AI paradigm for the present project was further motivated by the following considerations:

-   Real-time data is likely sparse in clinical settings. Extracting the maximum amount of information from each data point will therefore be essential. Using models that perform optimal Bayesian inference would therefore be preferable.

-   Short-term exploitation of tumor vulnerability is essential for keeping the patient alive, but for long-term success, controlling resistance dynamics will be crucial. A therapy plan will have to balance keeping the patient alive now against limiting its future options for treatment. Using a measure such as Expected Free Energy could be instrumental in striking this balance.

-   Resistance dynamics are not directly measurable, but should be inferable through observing how the tumor burden responds to treatment. Choosing whether to treat would therefore not only be a consideration of the tumor level but also about how much it informs us about the resistance dynamics.

-   The information gain from testing can be quantified and used to prioritize testing for periods of greater uncertainty.

An interesting corollary to these motivations is that AI POMDPs could suggest some superficially counterintuitive treatment decisions. For example, a model could correctly estimate that one patient has a low tumor burden and suggest not applying treatment, while proposing treatment for another patient with exactly the same burden. This is a feature, not a bug. If the model has less certainty about the resistance dynamics of the second patient, applying treatment could be the optimal move simply to gain information about the resistance dynamics of the cancer in the second patient.

# Analysis

## Simulated environment

To investigate the appropriateness of the AI implementation of the POMDP scheme, multiple simulations were run. To easily comply with the discrete state and time implementation typical of the AI literature, discrete states and time were used. In each simulation run, a simulated clinical setting of a maximum of 200 timesteps was created. In each timestep, a POMDP has to keep a virtual cancer patient alive by deciding when to apply a treatment based on a signal of the tumor burden. This environment was inspired by the use of PSA testing and Abiraterone treatment in the pilot clinical trial [@zhang2017]. In the simulations, a patient's *tumor state* determines whether they survive to the next timestep. The simulation ends if the patient does not survive. Simulation runs always begin with the tumor state at 0, but it increases with a fixed risk at each timestep. If the tumor state reaches 5, the virtual patient "dies" and the simulation ends.

Applying treatment can reduce the tumor state and thus prevent the patient from dying. However, whether a treatment successfully reduces the tumor state depends on an underlying *resistance state*, which also begins at state 0 out of 5. When the resistance is low, the chance of treatment succeeding is high. But for each round of treatment application, the resistance state has a fixed risk of increasing. This renders treatment less and less effective. The resistance state can decrease if treatment is withdrawn, and the probability of the resistance state decreasing depends on the tumor level. At high tumor levels, the resistance state is most likely to decrease. This is based on the work of [@hansen2020], who suggests that larger tumor sizes should generate more competitive pressure on resistant cells.

All in all, this means that a model has to balance the tumor state not growing out of control, which would "kill the patient," while not painting itself into a corner by applying treatment too frequently.

## Modifications to POMDP scheme

Typically for AI implementations of POMDPs, different types of hidden states such as resistance and tumor states are not modeled to affect each other directly. Instead, the interactions of combinations of different states are modeled to lead to different observations. This means that their interactions would be coded in the $A$-array, which contains the likelihood mappings.

However, this setup was deemed incompatible with the clinical setting attempted emulated. Therefore, custom changes to the implementation in pymdp were made to allow a POMPD to infer causally upstream "occluded" nodes, which don't themselves produce observations (E.g the resistance state). Instead the state of the occluded node will have to be inferred, through its downstream effects on partially observable state (the tumor state).

Specifically, modifications were made to the structure of the generative model's transition beliefs. Usually, three-dimensional "B-tensors" describe expected transition probabilities for a hidden state. This would mean a B-tensor would hold one dimension for the current state, one dimension for the next state, and a third for every allowed action. These B-tensors were given another dimension that corresponds to the state of another type of hidden state. Concretely, this meant that the tumor state factor had a fourth dimension corresponding to tumor transition probabilities for each resistance state. The expected transition probabilities could then be estimated by matrix multiplication of the B-tensor containing the tumor state transition probabilities and a vector of expected resistance levels at a given time point. One can imagine that instead of having only one B-tensor for the transition probabilities of the tumor state, multiple B-tensors of tumor transition rates were specified. The transition probabilities of each tensor then corresponded to a specific resistance level. This modification to the generative model necessitates taking an average of all these tensors weighted by the expected probabilities for each resistance state. This amounts to calculating the expected tumor state conditional on a set of beliefs about the resistance state and whether treatment was being applied. This modification strategy was repeated for the resistance level since decreases in the resistance state depended on the tumor state. While these changes rendered much of the functionality of pymdp immediately unusable, this strategy should be able to model arbitrarily complex dependencies between states. Hopefully, a robust implementation can be designed without overhauling pymdp. It should also be noted that this modification means that it is no longer meaningless in which order beliefs about states are evaluated. Instead, this must be specified. For example, for the present simulations, the effect of resistance level on treatment efficacy was evaluated first since this was evaluated first at each step of the simulation.

## Exploratory Simulation

An exploratory simulation was conducted, where a single run of POMDP controlling treatments and testing was performed to investigate the feasibility of using the modified POMDP scheme. This model could choose whether to test and treat. It would always observe a perfect signal of whether the patient is alive, whether the model is testing, and whether treatment is being applied. If the model decided to test, it would receive a signal of the tumor state. Its prior preferences were heavily against observing a dead patient, somewhat against treating, and slightly against testing. This was done to simulate a cost to both treating and testing. This means that the model will have to balance the utility gained and the "cost" of all these actions while considering the potential information gain of each choice. The model was further handicapped by adding noise to the tumor signal. This means that the resistance state, which must be inferred through the tumor signal, is doubly obfuscated. However, the likelihood mapping accounted for the expected noise in the tumor signal, and it perfectly knew the transition probabilities of the environment. The model was given uniform priors over initial states, meaning it had no knowledge at the beginning of each run. The model evaluated the same set of predetermined policies considering `r num_policies` timesteps into the future. This was done to ease computation by limiting the search space. The policies consisted of two blocks of either testing or treating for three timesteps in a row for each combination of applying testing or not at the timestep.

### Learning underlying hidden state

The simulation run managed to keep the patient alive for `r exploratory_sim_length` timesteps and chose to test on `r num_tests` of these (see Fig. 1).

```{r fig.cap="Figure 1. Shows a simulation run for exploratory analysis."}
source("sim_run_example.R")
plot
```

The model seems to be somewhat sensibly applying treatment. Generally, it refrains from applying treatment when the tumor state is low. Interestingly, it applies treatment at the first timestep, despite the tumor state and resistance state being at their lowest possible values. Considering that the model has uniform priors about the resistance state and tumor level, this means that it is maximally uninformed about both. Treating and testing would therefore immediately provide information about both. Qualitatively, the model also seems to prefer testing when treatment is being applied. This suggests that it finds testing to be more worthwhile while also treating. Considering that testing during treatment not only provides information about the tumor state but also the resistance state, this behavior is useful.

A plot of the model's beliefs about the resistance state at each time-point (see Fig. 2) also shows that its beliefs generally follow the development of the actual resistance level.

```{r, fig.cap="Figure 2. Plot of the models belief about the reistance level at every timepoint"}

source("resistance_beliefs_plot.R")
plot 
```

All in all, the modification to POMPD structure seems to allow the model to infer an the underlying state of causally upstream state factor through a noised signal form of a downstream node. This suggests that the model could be used to infer the resistance state from the tumor state. However, the model's behavior is not yet optimal.

### Expected trajectories

For each policy, the model evaluates the expected trajectory of the hidden states (see Fig. 3). These evaluations provide insights into the model's uncertainty and what it expects to happen under the best policy at each time point.

```{r}
source("single_model_trajectories plot.R")
plot
```

The amount of "flip-flopping" on decisions can also be investigated by examining what policy the model finds the most promising at each time step (see Fig. 4).

```{r}
source("single_run_overall_fep_policies.R")
plot
```

Curiously, there is a long stretch, approximately from timesteps 30 to 39, where the model never considers treating to be the optimal course of action, even though the resistance level is low. The model also holds accurate beliefs about the resistance level at this timepoint (see Fig. 2). However, this is likely due to a combination of multiple factors. First, there is a cost to treating, which means that the model generally prefers not to treat. It also underestimates the tumor burden at this timepoint (see Fig. 5) and gets "caught off guard" by a sudden rise in the tumor level. Since there is a cost associated with testing, it is also hesitant to apply tests during this period. Because it only considers six timesteps into the future, it likely doesn't consider the long-term consequences of not reducing the tumor level.

```{r}
source("SingleTrial_tumor_beliefs.R")
plot

```

The exact "decision making" for each policy can be evaluated. Each policy is evaluated for its EFE, which is a combination of the expected utility penalized for expected uncertainty (see Fig. 6).

```{r}

source("FEPcomponents.R")
plot
```

The free energy components reveal several interesting decision-making points. At first, the model, given its uniform priors, is very uncertain about outcomes, and decreasing uncertainty weighs heavily in its decision making. This uncertainty also means that the model believes it is more likely to be in a risky situation, and reducing this risk is a priority. During the earlier identified pivotal period between timesteps 30-39, there is a concerning lack of increase in the weight of the utility component. This could mean that the model is too concerned with avoiding both treating and testing. It is only from timestep 40 onwards that the model realizes the severity of the situation, i.e., the rising tumor level (see Fig. 5). This analysis warrants strategies to increase the model's ability. Changing the prior preferences to make testing and treatment "cheaper" in terms of utility might help. However, simply treating and testing more would translate to actual costs, both financial and to patient well-being. While it is important that the prior preferences are tuned to costs in a clinical setting, it would probably be more fruitful to give the model longer policies to consider. This would hopefully yield models that are better tuned to the risks of not detecting and controlling tumor levels at low values.

## Performance of POMPDs against a Range-Bounded Strategy

Several modified AI POMPDs were tested against a range-bounded strategy to benchmark the performance of POMPDs against a strategy inspired by the treatment protocol used in the pilot clinical trial [@zhang2017]. The POMPDs were modified so that they could only observe the tumor state, receiving a noiseless signal of this at every timestep. They were also only given prior preferences for observations of tumor states. These preferences were uniform over all states except for the highest tumor state, for which the probability was set to 0. Three POMPDs were tested with different lengths of policy horizons:

-   A short horizon model that could consider a block of treating or not treating for the next three steps.
-   A medium horizon model that could consider two blocks of treating or not treating for three steps, yielding a total horizon of 6 timesteps.
-   A long horizon model that could consider four blocks of treating or not treating for three steps, resulting in a total horizon of 12 steps.

These three models were benchmarked on four different "tumor aggressivity settings" against a range-bounded strategy inspired by the pilot clinical trial. The tested growth rates were 0.1, 0.3, 0.5, and 0.7 probability of increasing the tumor state at each timestep. Each of these four bouts consisted of 100 simulation runs for each model. For each single run, the environment was predetermined by generating 200 outcomes at each combination of tumor state, resistance state, and treatment state. This was done to ensure comparability at the level of each run.

```{r, fig.cap= "Figure. 7. The observed simulation lengths for each model type and tumor risk."}
#print densities
source("sim_lengths_densities_full.R")

plot + 
  #set color scheme so that long _vs_adt is green
  scale_fill_manual(values = strat_colors) +
  scale_color_manual(values = strat_colors) 



```

At the lowest tumor growth rate, nearly all the runs reached the maximal length (see Fig. 7). The longest policy strategy also appeared to perform slightly better than the other models. Since the difficulty of managing the tumor burden changed, contrasts were computed between the long policy POMPD and the range-bounded strategy. Because the stochasticity of each run was predetermined, the percentage difference from switching from an RB-strategy to a POMPD can be computed for each run (see Fig. 8).

```{r, fig.cap="Figure 8. Shows the change in percent of between the range-bound approach and hte longest horizon POMPD for tumor aggresivity settings"}
source("contrasted_sim_length_max_horizon.R")

plot +
  scale_fill_manual(values = contrast_colors) +
  scale_color_manual(values = contrast_colors) +
  xlim(-50, 400)

```

In the 100 runs with a 0.1 risk of tumor growth, the long horizon POMPD seems to perform similarly to the RB strategy. However, as the tumor growth risk increases, the POMPD model increasingly shows large percentage improvements compared to the RB strategy. This could be because, as the tumor dynamics speed up, the need for proactive action becomes more important, favoring models that attempt to also control the underlying resistance state.

# Discussion

## What This Project Found

This project tested a simple modification to how POMDPs are typically implemented in AI to investigate whether POMDPs could learn and control a hidden state that could only be inferred based on its consequences on other states. This was done in a simulated environment designed to mimic the dynamics of AT. The modification was successful, allowing the POMDP to model an underlying resistance state that controlled the efficacy of treatment, despite the underlying state not producing any observable signals itself. POMPDs were further tested against a range-bounded treatment strategy and were found to outperform the range-bounded strategy. The difference in performance increased as the aggressiveness of tumor growth increased and as the future time-horizon that the POMPDs considered at each time step lengthened. This finding underscores the potential of using the AI paradigm to plan AT, and particularly demonstrates that, under simplified circumstances, POMDPs could be a useful choice for implementing AI for real-time treatment decision-making.

Additionally, this paper demonstrates that the AI implementation of POMPDs shows several desirable qualities for AT POMDPs:

-   The models are flexible. Computation and our ability to inform likelihood and transition dynamics are the only limits to the combinations of hidden states, actions, and outcome modalities. This paper implemented a POMPD capable of combining multiple decisions, such as treating and testing, but more complex generative models could include multiple testing and treatment actions. As [@west2023] highlight, it is important to incorporate biological factors into modeling decisions.

-   Another quality is the capacity to balance expected information gain against expected utility. The certainty of the model is incorporated directly in selecting when to apply treatments and when to test. This suggests that there is more information to be gained from testing when treatment is being applied, as it provides a view into the underlying resistance dynamics. Immediate testing seems to suggest that the models are more likely to apply testing when also applying treatment. Another interesting corollary is that a POMPD could believe that two patients have exactly the same tumor burden but suggest treatment for one and not the other. This is a feature, not a bug. If the model is certain about an underlying state (e.g., resistance in the models in this paper), there is less gain in information from treating. On the other hand, if the model is uncertain about the resistance state, it might be worthwhile to treat simply to gain information. This is appropriate behavior, as it allows the model to better its chances of keeping the patient alive in the future.

-   Another quality desired [@west2023] is rigorous uncertainty. Since the POMDP's certainty about current and future states is easily extractable, this criterion is arguably met. Additionally, its "reasoning," i.e., free energy estimates, can be examined for each action.

## Future Research

While the results are initially promising, the simple nature of the simulation prohibits drawing any strong conclusions about whether AI POMPDs could eventually be a good model choice in a clinical setting.

To further investigate whether POMDPs could apply to a clinical setting, multiple problems will need to be solved. For example, in the present study, the environment featured only discrete values. Since biomarkers and dosage intensities will likely be continuous values, some combination of binning continuous values to discrete values or adapting the model structure to work with continuous data is necessary. Deep learning has been used to construct likelihood mappings and transition probabilities in POMDPs from continuous data [@çatal2020]. Another key issue is determining a way to inform likelihood and transition probabilities in a fashion that would be viable in a clinical setting. The present study simply used the actual transition probabilities of the simulated environment and, depending on the simulation run, also used noiseless likelihood mappings. Possibly, simulations of cancer dynamics, such as those by [@zhang2017], could be used to inform transition probabilities, and a clinical model could potentially readjust to patient data. The AI implementation of POMDPs has a developed literature on learning probabilities from data and even on how to incorporate the information gain of learning transition and likelihood probabilities into decision-making ([@smith2022] and @dacosta2020]).

Another key issue is constructing a more rigorous benchmarking system for proposed models. In the present paper, the ranges of the compared range-bounded model were selected fairly arbitrarily. It produced convincing results during initial testing and did manage to control tumor levels for a substantial amount of time. However, a systematic method of comparing models is necessary, especially considering the difficulty of real-world tests. No matter the choice of modeling framework, building a benchmarking suite is crucial for the AT discipline. Given the difficulty of real-world testing, we must maximize the information that can be extracted from simulation work. [@west2023] has produced a detailed qualitative account of needed developments in mathematical models. Translating this account to a set of simulation environments would be extremely useful. A number of useful simulation scripts probably already exist. For example, [@zhang2017] released their simulations in Matlab. Wrapping existing simulations into environments that can easily exchange actions and observations could be facilitated by using an API like the one used in Gymnasium (formerly OpenAI Gym) [@towers2024] to minimize friction for non-oncology researchers. If experts in AT were to predetermine a set of benchmarks, it would also greatly ease the burden on outsiders to identify useful contributions, allowing them more time to implement these contributions.

If an easy-to-use benchmarking suite existed, POMPDs could also be "reverse-engineered" from more complex models. If a black-box model, like a neural network, can be shown to successfully control treatment application in more complicated simulations of cancer dynamics, this would presumably not be too difficult considering that POMPDs have often been fitted to human decision-making in computational psychiatry. The same techniques could potentially allow us to translate the decision-making of a black-box model into the structure of a POMPD, thus combining the performance of the black-box model with the transparency of POMPD structure.
